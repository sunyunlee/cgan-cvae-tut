{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, noise_dim, label_dim):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(noise_dim + label_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden2 = nn.Linear(10, 10)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.hidden3 = nn.Linear(10, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, noise, label):\n",
    "        out = F.relu(self.input(torch.cat((noise, label), -1)))\n",
    "        out = F.relu(self.hidden(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.hidden2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = F.relu(self.hidden3(out))\n",
    "        out = self.output(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, label_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_dim + label_dim, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden2 = nn.Linear(10, 10)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.hidden3 = nn.Linear(10, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        out = F.relu(self.input(torch.cat((x, label), -1)))\n",
    "#         out = self.batch_norm(F.relu(self.hidden(out)))\n",
    "        out = F.relu(self.hidden(out))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.hidden2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = F.relu(self.hidden3(out))\n",
    "        out = self.output(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  Price  \n",
      "0     15.3  396.90   4.98   24.0  \n",
      "1     17.8  396.90   9.14   21.6  \n",
      "2     17.8  392.83   4.03   34.7  \n",
      "3     18.7  394.63   2.94   33.4  \n",
      "4     18.7  396.90   5.33   36.2  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset \n",
    "bos = load_boston()\n",
    "df = pd.DataFrame(bos.data)\n",
    "df.columns = bos.feature_names\n",
    "df[\"Price\"] = bos.target\n",
    "\n",
    "# # Standardize\n",
    "# data = df[df.columns[:-1]]\n",
    "# data = data.apply(\n",
    "#     lambda x: (x - x.mean()) / x.std()\n",
    "# )\n",
    "\n",
    "data['Price'] = df.Price\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "# Define the models\n",
    "# Dataset to tensor \n",
    "# \n",
    "X = torch.tensor(data.drop(\"Price\", axis=1).values).type(torch.float64)\n",
    "Y = torch.tensor(data[\"Price\"].values).type(torch.float64).view(-1, 1)\n",
    "\n",
    "X_TRAIN, X_test, Y_TRAIN, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "Y_TRAIN = Y_TRAIN.view(-1, 1)\n",
    "Y_test = Y_test.view(-1, 1)\n",
    "\n",
    "# Standardize data \n",
    "scaler = preprocessing.StandardScaler().fit(X_TRAIN)\n",
    "X_TRAIN = scaler.transform(X_TRAIN)\n",
    "    # Should not t\n",
    "    # Want to use the same standard deviation and mean as the train dataset because our model thinks\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "X = X_train + X_test\n",
    "(X - mean(X)) / std(X) -- This is bad because we are introducing information about \"future\" data into our training set \n",
    "\n",
    "X_train = (X_train - mean(X_train)) / std(X_train) \n",
    "X_test = (X_test - mean(X_test)) / std(X_test)\n",
    "-- Think of test set as a data that we are making prediction for (as if we didn't have y values).\n",
    "You can see why we would need to apply the normalization scales of train data to test data. \n",
    "You don't get an entire set of input data all at once, same model can be used many times on different input values. Then we don't really know what the mean and standard deviation of all inputs are. \n",
    "We assume that the data that we get are from the same scales as the test data and use its scale values.\n",
    "Always assume that the model does not know the distribution (or in this case scale) of the data that it needs to make pre\n",
    "\n",
    "Correct way \n",
    "X_train = (X_train - mean(X_train)) / std(X_train)\n",
    "X_test = (X_test - mean(X_train)) / std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters \n",
    "N_EPOCHS = 40\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "INPUT_DIM = X.shape[1]\n",
    "LABEL_DIM = Y.shape[1]\n",
    "NOISE_DIM = 5\n",
    "HIDDEN_DIM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "gen = Generator(INPUT_DIM, HIDDEN_DIM, NOISE_DIM, LABEL_DIM).type(torch.float64)\n",
    "dis = Discriminator(INPUT_DIM, LABEL_DIM, HIDDEN_DIM).type(torch.float64)\n",
    "\n",
    "prior = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers \n",
    "optG = torch.optim.Adam(gen.parameters(), lr=lr, weight_decay=1e-4)\n",
    "optD = torch.optim.Adam(dis.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "logsigmoid = torch.nn.LogSigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Cross Validation \"\"\"\n",
    "# Leave one out \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "print(loo.get_n_splits(X_TRAIN))\n",
    "print(len(X_TRAIN))\n",
    "\n",
    "\n",
    "# K fold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 40] average reconstruction error: 0.500000\n",
      "Epoch [2 / 40] average reconstruction error: 0.500000\n",
      "Epoch [3 / 40] average reconstruction error: 0.500000\n",
      "Epoch [4 / 40] average reconstruction error: 0.500000\n",
      "Epoch [5 / 40] average reconstruction error: 0.500000\n",
      "Epoch [6 / 40] average reconstruction error: 0.500000\n",
      "Epoch [7 / 40] average reconstruction error: 0.500000\n",
      "Epoch [8 / 40] average reconstruction error: 0.500000\n",
      "Epoch [9 / 40] average reconstruction error: 0.500000\n",
      "Epoch [10 / 40] average reconstruction error: 0.500000\n",
      "Epoch [11 / 40] average reconstruction error: 0.500000\n",
      "Epoch [12 / 40] average reconstruction error: 0.500000\n",
      "Epoch [13 / 40] average reconstruction error: 0.500000\n",
      "Epoch [14 / 40] average reconstruction error: 0.500000\n",
      "Epoch [15 / 40] average reconstruction error: 0.500000\n",
      "Epoch [16 / 40] average reconstruction error: 0.500000\n",
      "Epoch [17 / 40] average reconstruction error: 0.500000\n",
      "Epoch [18 / 40] average reconstruction error: 0.500000\n",
      "Epoch [19 / 40] average reconstruction error: 0.500000\n",
      "Epoch [20 / 40] average reconstruction error: 0.500000\n",
      "Epoch [21 / 40] average reconstruction error: 0.500000\n",
      "Epoch [22 / 40] average reconstruction error: 0.500000\n",
      "Epoch [23 / 40] average reconstruction error: 0.500000\n",
      "Epoch [24 / 40] average reconstruction error: 0.500000\n",
      "Epoch [25 / 40] average reconstruction error: 0.500000\n",
      "Epoch [26 / 40] average reconstruction error: 0.500000\n",
      "Epoch [27 / 40] average reconstruction error: 0.500000\n",
      "Epoch [28 / 40] average reconstruction error: 0.500000\n",
      "Epoch [29 / 40] average reconstruction error: 0.500000\n",
      "Epoch [30 / 40] average reconstruction error: 0.500000\n",
      "Epoch [31 / 40] average reconstruction error: 0.500000\n",
      "Epoch [32 / 40] average reconstruction error: 0.500000\n",
      "Epoch [33 / 40] average reconstruction error: 0.500000\n",
      "Epoch [34 / 40] average reconstruction error: 0.500000\n",
      "Epoch [35 / 40] average reconstruction error: 0.500000\n",
      "Epoch [36 / 40] average reconstruction error: 0.500000\n",
      "Epoch [37 / 40] average reconstruction error: 0.500000\n",
      "Epoch [38 / 40] average reconstruction error: 0.500000\n",
      "Epoch [39 / 40] average reconstruction error: 0.500000\n",
      "Epoch [40 / 40] average reconstruction error: 0.500000\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = []\n",
    "\n",
    "\n",
    "for ep in range(N_EPOCHS):\n",
    "    train_accuracy.append(0)\n",
    "    num_batches = 0 \n",
    "    for train_index, test_train_index in loo.split(X_TRAIN):\n",
    "#     for train_index, test_train_index in kf.split(X_TRAIN):\n",
    "        dis.train()\n",
    "        gen.train()\n",
    "        X_train, X_test_train = X_TRAIN[train_index], X_TRAIN[test_train_index]\n",
    "        Y_train, Y_test_train = Y_TRAIN[train_index], Y_TRAIN[test_train_index]\n",
    "        \n",
    "        N = len(X_train)\n",
    "        \n",
    "        \"\"\" Train Discriminator \"\"\"\n",
    "        # zero grad \n",
    "        optD.zero_grad()\n",
    "        \n",
    "        # Forward pass \n",
    "        # 1. Discriminator \n",
    "        outD = dis(X_train, Y_train)\n",
    "        \n",
    "        # 2. Generator-Discriminator\n",
    "        noise_dis = prior.sample((N, NOISE_DIM)).type(torch.float64)\n",
    "        outG_dis = gen(noise_dis, Y_train)\n",
    "        outDG_dis = dis(outG_dis, Y_train)\n",
    "        \n",
    "        # Loss\n",
    "        lossD = -torch.mean(logsigmoid(outD) + logsigmoid(1 - outDG_dis))\n",
    "        \n",
    "        # Backward\n",
    "        lossD.backward()\n",
    "        \n",
    "        # Update parameters \n",
    "        optD.step()\n",
    "                \n",
    "        \"\"\" Train Generator \"\"\"\n",
    "        # zero grad \n",
    "        optG.zero_grad()\n",
    "        \n",
    "        # Forward pass \n",
    "        noise_gen = prior.sample((N, NOISE_DIM)).type(torch.float64)\n",
    "        outG_gen = gen(noise_gen, Y_train)\n",
    "        outDG_gen = dis(outG_gen, Y_train)\n",
    "        \n",
    "        # Loss \n",
    "        lossG = -torch.mean(logsigmoid(outDG_gen))\n",
    "        \n",
    "        # Backward \n",
    "        lossG.backward()\n",
    "        \n",
    "        # Update parameters \n",
    "        optG.step()\n",
    "        \n",
    "        \"\"\" Train Accuracy (Leave one out) \"\"\"\n",
    "        dis.eval()\n",
    "        gen.eval()\n",
    "        # Forward pass\n",
    "        # 1. Discriminator\n",
    "        outD = dis(X_test_train, Y_test_train)\n",
    "        outD = torch.nn.Sigmoid()(outD[0,0])\n",
    "#         outD = torch.nn.Sigmoid()(outD)\n",
    "        outD_acc = int((outD > 0.5) == 1)\n",
    "#         outD_acc = torch.sum((outD > 0.5) == 1)\n",
    "        \n",
    "        train_accuracy[-1] += outD_acc \n",
    "        num_batches += 1\n",
    "#         num_batches += X_test_train.shape[0]\n",
    "        \n",
    "\n",
    "        # 2. Generator-Discriminator\n",
    "        noise = prior.sample((Y_test_train.shape[0], NOISE_DIM)).type(torch.float64)\n",
    "        outG = gen(noise, Y_test_train)\n",
    "        outDG = dis(outG, Y_test_train)\n",
    "        outDG = torch.nn.Sigmoid()(outDG[0,0])\n",
    "#         outDG = torch.nn.Sigmoid()(outDG)\n",
    "        outDG_accuracy = int((outDG > 0.5) == 0)\n",
    "#         outDG_accuracy = torch.sum((outDG > 0.5) == 0)\n",
    "        \n",
    "        train_accuracy[-1] += outDG_accuracy\n",
    "        num_batches += 1     \n",
    "#         num_batches += X_test_train.shape[0]\n",
    "    \n",
    "    train_accuracy[-1] /= num_batches\n",
    "#     train_accuracy[-1] = train_accuracy[-1].item() / num_batches\n",
    "\n",
    "    print(\"Epoch [%d / %d] average reconstruction error: %f\" % (ep+1, N_EPOCHS, train_accuracy[-1]))\n",
    "    \n",
    "    # https://arxiv.org/pdf/1802.03446.pdf\n",
    "    # https://arxiv.org/pdf/1610.06545.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_accuracy)\n",
    "plt.axhline(0.5, linestyle='--')\n",
    "plt.title(\"Train Accuracies\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy on held out data:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Test Test \n",
    "outD = dis(X_test, Y_test)\n",
    "\n",
    "noise = prior.sample((len(X_test), NOISE_DIM)).type(torch.float64)\n",
    "outG = gen(noise, Y_test)\n",
    "outDG = dis(outG, Y_test)\n",
    "\n",
    "output = torch.cat((outD, outDG))\n",
    "label_real = torch.ones(outD.shape)\n",
    "label_fake = torch.zeros(outDG.shape)\n",
    "labels = torch.cat((label_real, label_fake))\n",
    "\n",
    "accuracy = torch.mean(((output > 0.5) == labels).type(torch.float64)).item()\n",
    "\n",
    "print(\"Final test accuracy on held out data: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
